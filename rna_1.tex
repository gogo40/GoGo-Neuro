\documentclass[10pt,journal,letterpaper,compsoc]{IEEEtran}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{multirow}

\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

\makeatletter
%%\usepackage{babel}
\usepackage[brazilian]{babel}
\makeatother


\newcommand{\inte}[4]{
	\displaystyle \int_{#1}^{#2} #3 d#4
}

\newcommand{\fra}[2]{
	\displaystyle \frac{{#1}}{{#2}} 
}
\newcommand{\frap}[2]{
	\left( \displaystyle \frac{#1}{#2} \right)
}

\newcommand{\su}[2]{
	\displaystyle \sum_{#1}^{#2}
}

\newcommand{\df}[2]{
	\fra{d \left[ #1 \right]}{d#2}
}

\newcommand{\dfa}[2]{
	\fra{d #1 }{d#2}
}

\newcommand{\dpa}[2]{
	\fra{\partial #1 }{\partial #2}
}

\title{$9^o$ Trabalho de Redes Neurais}
\author{Péricles Lopes Machado}

\begin{document}

\maketitle

\begin{abstract}

Neste trabalho, algumas rede neurais RBF são treinadas para simular a equação de recorrência:
\begin{equation}
x_k = x_{k-1} + \fra{a x_{k-s} } { 1 + x_{i-s} ^ c} - b x_{k-1} + 0.1 N ,
\label{eq.1}
\end{equation}
onde $N$ é uma variável aleatória
de distribuição normal com $\mu = 0$ e $\sigma^2 = 1$, $a = 0.2$,
$b = 0.1$, $c = 10$ e $s = 17$. Além disso, é realizado um estudo, utilizando
a função de auto-correlação do sinal $x_k$,
para se tentar obter a melhor escolha
de parâmetros anteriores da série para minimizar o erro quadrático associado a rede.

\end{abstract}

\section{As rede neurais}

A rede neural possuiam 7 neurônios, cujos centros associados eram escolhidos de forma aleatória
dentro da base de dados filtrada contendo em torno de 3 mil amostras da série $x_k$. A distância $D$,
utilizada para avaliar a distância entre uma entrada e o centro associado ao neurônio, foi a euclidiana.
A função de ativação utilizada foi $u_i(\vec{x}) =  e^{-\beta  D(\vec{x} - \vec{c_i})}$, onde $c_i$ é
o centro associado ao neurônio $i$ e o parâmetro livre $\beta$, nos testes realizados, foi $0,1$.

\section{O tipos de entrada da rede}
Foram definidos cinco tipos de entradas para a rede:

\begin{itemize}
\item $I_1 = \{x(k-1)\}$
\item $I_2 = \{x(k-1), x(k-2)\}$
\item $I_3 = \{x(k-1), ... , x(k-3)\}$
\item $I_4 = \{x(k-1), ... , x(k-4)\}$
\item $I_5 = \{x(k-a), ... , x(k-b)\}$
\end{itemize}

No caso do $I_5$, tenta-se definir a melhor escolha de $a$ e $b$ que minimiza o erro quadrático.
A função de auto-correlação é utilizada na tentativa de se determinar a melhor escolha para $a$ e
$b$. 

Para cada tipo de entrada foi gerado um banco de dados de treino e teste utilizando-se o mesmo algoritmo
de filtragem apresentado nos trabalhos 7 e 8.

\section{O treinamento}

Como no trabalho 8, o método de mínimos quadrados foi utilizado para o ajuste dos pesos de cada rede.

\section{Resultados}

Utilizando a rede RBF foi possível reduzir o erro quadrático 
no teste para $0,099612$, um resultado próximo ao 
obtido com o {\it backpropagation} no trabalho anterior (0,098). Mas, para se 
obter esse erro, o tempo de treinamento
foi bem reduzido, já que o {\it mínimos quadrados} não sofre com problemas de mínimos locais que afetam
significativamente o método {\it backpropagation}.

Este experimento ilustra uma característica importante da técnica de redes neurais com funções de base radial,
o ajuste dos pesos é bem mais simples e é possível obter resultados muito bons em um tempo de treinamento 
reduzido.
No caso, o treinamento se resume na filtragem da {\it database} e no cáculo da {\it pseudo-inversa} da matriz $w$.

A figura \ref{erro.1} ilustra os bons resultados obtidos.

\begin{figure}[!htb]
     \centering
     \includegraphics[scale=0.5]{./t8/saidaRBFt.png}
     \caption{Comparação da saída obtida pela rede RBF com a saída esperada.}
     \label{erro.1}
\end{figure}


\section{Conclusão}
Neste trabalho, uma rede neural utilizando funções de base radial é utilizada para simular a mesma série temporal
do trabalho 7. Com um tempo de processamento bem menor e utilizando a mesma {\it database} e a mesma quantidade 
de neurônios foi possível ajustar os pesos da rede para se obter um erro similar ao obtido no trabalho anterior.
As principais dificuldades na utilização dessa técnica são a escolha de um valor apropriado para o parâmetro livre
{\it beta} e a determinação dos centros associados a cada neurônio da rede.

Além disso, mostrou-se como técnicas de treinamento sem a utilização do gradiente do erro podem ser utilizadas
para um ajuste rápido dos pesos de uma rede neural. Embora, a técnica requeira mais espaço de memória, isso é
facilmente compensado pelo fato do tempo de ajuste dos pesos ser bastante reduzido.

\end{document}

